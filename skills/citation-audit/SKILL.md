---
name: citation-audit
description: >
  Systematic audit of academic manuscript references: authenticity verification,
  bibliographic accuracy, citation appropriateness, and software/data version
  consistency. Triggers on: citation audit, reference check, bibliography
  verification, fabricated/fake/hallucinated reference detection, DOI verification,
  pre-submission check, manuscript review, R/Python package version consistency,
  data source citation, checking if a paper is real, æ–‡çŒ®å®¡æŸ¥, å¼•ç”¨å®¡æŸ¥, å‚è€ƒæ–‡çŒ®æ£€æŸ¥,
  ä¼ªé€ æ–‡çŒ®, æŠ•ç¨¿å‰æ£€æŸ¥, DOIæ ¸å¯¹, è½¯ä»¶ç‰ˆæœ¬æ ¸å¯¹, æ•°æ®æºå¼•ç”¨.
  Applicable file types: .docx, .tex, .bib, .ris, .enl, .nbib, manuscript files.
---

# Citation Audit / å­¦æœ¯æ–‡çŒ®å®¡æŸ¥

Systematic audit of all references in an academic manuscript before submission.
æŠ•ç¨¿å‰å¯¹å­¦æœ¯ç¨¿ä»¶å‚è€ƒæ–‡çŒ®è¿›è¡Œç³»ç»Ÿæ€§å…¨é¢å®¡æŸ¥ã€‚

## Audit Dimensions / å®¡æŸ¥ç»´åº¦

| Level | Scope / èŒƒå›´ | Severity / ä¸¥é‡åº¦ |
| ----- | ------------ | ----------------- |
| **L1** | Authenticity â€” does the paper exist? Is the DOI correct? / çœŸå®æ€§â€”â€”è®ºæ–‡æ˜¯å¦å­˜åœ¨ï¼ŸDOI æ˜¯å¦æ­£ç¡®ï¼Ÿ | ğŸ”´ Fatal |
| **L2** | Bibliographic accuracy â€” authors, year, volume, pages, journal / ä¹¦ç›®ä¿¡æ¯â€”â€”ä½œè€…ã€å¹´ä»½ã€å·å·ã€é¡µç ã€æœŸåˆŠ | ğŸ”´ Critical |
| **L3** | Textâ€“list consistency â€” every in-text citation has a matching entry and vice versa / æ­£æ–‡ä¸åˆ—è¡¨ä¸€è‡´æ€§ | ğŸŸ¡ Important |
| **L4** | Citation appropriateness â€” each citation supports the claim it is attached to / å¼•ç”¨æ°å½“æ€§ | ğŸŸ¡ Improvement |
| **L5** | Formatting & version consistency â€” style uniformity, software/data versions match actual usage / æ ¼å¼ä¸ç‰ˆæœ¬ä¸€è‡´æ€§ | âšª Housekeeping |

## Workflow / å·¥ä½œæµç¨‹

### Phase 1: Extract manuscript text / æå–ç¨¿ä»¶å…¨æ–‡

Extract all text with paragraph indices for cross-referencing. See `scripts/extract_docx.py`.

For `.tex` files, parse directly. For `.docx`, use the python-docx library. Separate the **reference list** from the **body text** and index each entry.

### Phase 2: L1 â€” Authenticity verification / çœŸå®æ€§éªŒè¯

> [!CAUTION]
> AI-assisted writing frequently introduces "hallucinated" references â€” DOIs that resolve to unrelated papers, or entirely fabricated entries. This is the most severe error class.
>
> AI è¾…åŠ©å†™ä½œææ˜“å¼•å…¥"å¹»è§‰æ–‡çŒ®"ã€‚æ­¤ç±»é”™è¯¯ä¸€æ—¦å‘è¡¨åæœä¸¥é‡ã€‚

**Method / æ–¹æ³•: CrossRef API + web search dual verification**

1. Run `scripts/crossref_batch_check.py` to batch-query CrossRef API metadata.
2. **Mandatory web-search re-verification** for:
   - Entries where API results mismatch the manuscript
   - Connection errors or timeouts
   - Papers published within the last 1â€“2 years (CrossRef indexing lag)
   - Any citation that "looks too perfect" but cannot be independently found

**Red flags for fabricated references / ä¼ªé€ æ–‡çŒ®ç‰¹å¾:**

- DOI resolves to an unrelated paper
- Author + year + journal combination yields zero Google Scholar results
- Claims to cite a "preprint" but provides a formal journal DOI

**Verification chain for suspicious entries / å¯ç–‘æ¡ç›®éªŒè¯é“¾:**

1. Resolve DOI directly â†’ check title and author match
2. Google Scholar: search author + keywords
3. Author's personal page / ORCID publication list
4. Journal website: browse the table of contents for the cited volume/issue

### Phase 3: L2 â€” Bibliographic accuracy / ä¹¦ç›®ä¿¡æ¯æ ¸å¯¹

Check every entry against its verified source for:

| Field | Common errors / å¸¸è§é”™è¯¯ |
| ----- | ----------------------- |
| Authors | Missing co-authors (especially 4th+), wrong initials (G.H. vs C.H.) / é—æ¼åˆè‘—è€…ã€åç¼©å†™é”™è¯¯ |
| Year | Early Online vs. official publication date confusion / åœ¨çº¿ä¼˜å…ˆä¸æ­£å¼å‡ºç‰ˆæ—¥æœŸæ··æ·† |
| Journal | Abbreviated vs. full name inconsistency / ç¼©å†™ä¸ç»Ÿä¸€ |
| Volume/Pages | Mismatch with DOI record / ä¸ DOI è®°å½•ä¸ç¬¦ |
| DOI | Placeholder not replaced (e.g. `zenodo.XXXXXXX`), points to wrong article / å ä½ç¬¦æœªæ›¿æ¢ |

### Phase 4: L3 â€” Textâ€“list cross-check / æ­£æ–‡-åˆ—è¡¨äº¤å‰æ ¸å¯¹

1. Extract all `(Author, Year)` and `(Author et al., Year)` citations from the body text.
2. Match bidirectionally:
   - **In text â†’ not in list** = missing reference (must add) / ç¼ºå¤±å¼•ç”¨
   - **In list â†’ not in text** = orphan reference (delete or cite) / å¹½çµå¼•ç”¨
3. Special attention to data sources, software packages, and datasets that are mentioned in text but absent from the reference list.

### Phase 5: L4 â€” Citation appropriateness / å¼•ç”¨æ°å½“æ€§

Evaluate each citation:

- Does it directly support the claim it is attached to?
- Is there a more canonical or more recent alternative?
- Excessive self-citation or citation stacking?

### Phase 6: L5 â€” Formatting & version consistency / æ ¼å¼ä¸ç‰ˆæœ¬ä¸€è‡´æ€§

#### Style uniformity / æ ¼å¼ç»Ÿä¸€

- "et al." usage, punctuation, spacing
- Author name ordering for multi-work citations

#### Software & package version verification / è½¯ä»¶ç‰ˆæœ¬æ ¸å¯¹

> [!IMPORTANT]
> The manuscript MUST report the actual software versions used for the analysis, not the latest CRAN/PyPI versions.

**R environment:**

```r
pkgs <- c('ecospat', 'biomod2', 'terra', 'sf')
for (p in pkgs) cat(sprintf("%-12s %s\n", p, packageVersion(p)))
cat(sprintf("%-12s %s\n", "R", R.version.string))
```

**Python environment:**

```python
import pkg_resources, sys
for p in ['numpy', 'pandas', 'scikit-learn', 'tensorflow']:
    try: print(f"{p:20s} {pkg_resources.get_distribution(p).version}")
    except: print(f"{p:20s} NOT INSTALLED")
print(f"{'Python':20s} {sys.version.split()[0]}")
```

**Other environments** (Julia, MATLAB, etc.): adapt the pattern to query installed package versions.

**Cross-check steps:**

1. Search project scripts for all `library()` / `import` / `using` calls.
2. Query actual installed versions in the runtime environment.
3. Compare with versions stated in the manuscript and reference list.
4. Flag packages mentioned in the manuscript but never called in any script (may indicate a methodâ€“code mismatch).

#### Data source & dataset citation / æ•°æ®æºå¼•ç”¨æ ¸å¯¹

Verify that **every external data source** used in the analysis is properly cited:

| Data type | Examples | What to check |
| --------- | -------- | ------------- |
| Remote sensing | MODIS, Landsat, Sentinel | Product name, version, DOI or data center URL |
| Climate data | WorldClim, CHELSA, ERA5 | Version number, resolution, temporal coverage |
| Biodiversity records | GBIF, iNaturalist, VertNet | Download DOI, access date, query parameters |
| Geospatial layers | Natural Earth, GADM, OpenStreetMap | Version, access date |
| Genomic data | GenBank, SRA, ENA | Accession numbers |
| Statistical databases | World Bank, UN, national bureaus | Dataset name, access date, URL |

Common issues:

- Dataset is used in methods but has no reference entry
- DOI or accession number is a placeholder
- Version mismatch between what was downloaded and what is cited

## Output format / è¾“å‡ºæ ¼å¼

Generate a `citation_audit.md` report structured as:

```markdown
# Citation Audit Report / å‚è€ƒæ–‡çŒ®å®¡æŸ¥æŠ¥å‘Š

## ğŸ”´ Must-fix errors / å¿…é¡»ä¿®æ­£
(Ordered: fabricated > missing > bibliographic)

## ğŸŸ¡ Recommended improvements / å»ºè®®æ”¹è¿›
(Appropriateness, formatting)

## âœ… Verified entries / å·²éªŒè¯é€šè¿‡
(Full checklist with per-entry status)
```

## Key lessons / å…³é”®ç»éªŒ

1. **Never trust CrossRef alone** â€” its "best match" is frequently wrong for books, chapters, datasets, non-English literature, and same-surname authors. Always web-search verify.
   CrossRef è¿”å›çš„"æœ€ä½³åŒ¹é…"ç»å¸¸æ˜¯é”™è¯¯çš„ï¼Œå¿…é¡»ç”¨ Web æœç´¢äºŒæ¬¡éªŒè¯ã€‚

2. **Year discrepancies need judgment** â€” "Early Online" vs. print dates can differ by 1â€“2 years; both are acceptable. Differences > 2 years likely indicate a real error.
   å¹´ä»½å·®å¼‚éœ€åˆ¤æ–­ï¼šEarly Online ä¸æ­£å¼å‡ºç‰ˆå·® 1â€“2 å¹´å±æ­£å¸¸ã€‚

3. **Methods must match code** â€” if the manuscript claims package X was used but the scripts call package Y, this is a reviewable error. Cross-check Methods section against actual scripts line by line.
   ç¨¿ä»¶æ–¹æ³•æè¿°å¿…é¡»ä¸ä»£ç ä¸€è‡´ï¼Œéœ€é€è¡Œæ¯”å¯¹ã€‚

4. **Data sources need citations too** â€” remote sensing products, climate databases, and biodiversity data portals all require proper citation with DOI/version/access date.
   æ•°æ®æºä¹Ÿéœ€è¦è§„èŒƒå¼•ç”¨ã€‚

## Anti-patterns

| Don't / ä¸è¦ | Do instead / åº”è¯¥ |
| ------------ | ---------------- |
| Trust CrossRef blindly | CrossRef + web search dual verification |
| Ignore recent publications | Extra scrutiny for papers < 2 years old |
| Assume all DOIs are correct | Resolve every DOI and verify the target |
| Only check the reference list | Also cross-check body citations and code |
| Report everything at once | Triage by severity: fatal â†’ critical â†’ improvement |
| Skip data source citations | Verify every dataset, layer, and product is cited |
